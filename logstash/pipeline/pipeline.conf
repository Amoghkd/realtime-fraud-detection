input {
  kafka {
    bootstrap_servers => "kafka:9094"
    topics => ["alerts-high-value", "alerts-velocity", "alerts-impossible-travel", "alerts-ml"]
    group_id => "fraud_alerts_group"
    auto_offset_reset => "earliest"
    codec => "json"
  }
}

filter {
  # Aggregate events by `transaction_id`
  aggregate {
    task_id => "%{transaction_id}"
    code => "
      map['alerts'] ||= []
      map['alerts'] << event.to_hash
    "
    push_map_as_event_on_timeout => true
    timeout => 60
    timeout_tags => ["aggregated"]
    timeout_code => "
      if map['alerts'] && map['alerts'].size > 0
        event.set('transaction_id', map['alerts'][0]['transaction_id'])
        event.set('card_id', map['alerts'][0]['card_id'])
        event.set('amount', map['alerts'][0]['amount'])
        event.set('event_time', map['alerts'][0]['event_time'])
        event.set('ml_fraud_score', map['alerts'][0]['ml_fraud_score'])
        event.set('alert_types', map['alerts'].collect { |e| e['alert_type'] })
        event.set('reasons', map['alerts'].collect { |e| e['reason'] })
        event.set('alert_count', map['alerts'].size)
      else
        event.cancel()
      end
    "
  }
}

output {
  # We use a conditional check here to ensure we only send the final, aggregated events to Elasticsearch.
  # This prevents duplicate data and reduces the number of documents in your index.
  if "aggregated" in [tags] {
    stdout { codec => rubydebug }

    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "fraud-alerts-%{+YYYY.MM.dd}"
      # It's better to pre-define the index template outside of Logstash
      # to avoid the overhead of template management during runtime.
      manage_template => false
    }
  }
}